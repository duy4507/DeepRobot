{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SARSA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "CixOZgMjxZp7"
      },
      "source": [
        "#@title Install necessary dependencies.\n",
        "\n",
        "!sudo apt-get install -y xvfb ffmpeg\n",
        "!pip install 'gym==0.10.11'\n",
        "!pip install imageio\n",
        "!pip install PILLOW\n",
        "!pip install 'pyglet==1.3.2'\n",
        "!pip install pyvirtualdisplay\n",
        "\n",
        "!pip install dm-acme\n",
        "!pip install dm-acme[reverb]\n",
        "!pip install dm-acme[tf]\n",
        "!pip install dm-acme[envs]\n",
        "!pip install dm-acme[jax]\n",
        "!pip install dm-env\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzhxEJWdyEAG"
      },
      "source": [
        "#@title Import modules.\n",
        "#python3\n",
        "\n",
        "%%capture\n",
        "import copy\n",
        "import pyvirtualdisplay\n",
        "import imageio \n",
        "import base64\n",
        "import IPython\n",
        "\n",
        "\n",
        "from acme import environment_loop\n",
        "from acme.tf import networks\n",
        "from acme.adders import reverb as adders\n",
        "from acme.agents.tf import actors as actors\n",
        "from acme.datasets import reverb as datasets\n",
        "from acme.wrappers import gym_wrapper\n",
        "from acme import specs\n",
        "from acme import wrappers\n",
        "from acme.agents.tf import d4pg\n",
        "from acme.agents import agent\n",
        "from acme.tf import utils as tf2_utils\n",
        "from acme.utils import loggers\n",
        "\n",
        "import gym \n",
        "import dm_env\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import reverb\n",
        "import sonnet as snt\n",
        "import tensorflow as tf\n",
        "import acme\n",
        "\n",
        "# Set up a virtual display for rendering OpenAI gym environments.\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "4kYf0oulyVED",
        "outputId": "2b733ccc-86a1-4afe-f4f0-936c7c7e53cb"
      },
      "source": [
        "#@title Load environment\n",
        "environment_name = 'gym_mountaincar'  # @param ['dm_cartpole', 'gym_mountaincar']\n",
        "# task_name = 'balance'  # @param ['swingup', 'balance']\n",
        "\n",
        "def make_environment(domain_name='cartpole', task='balance'):\n",
        "  env = suite.load(domain_name, task)\n",
        "  env = wrappers.SinglePrecisionWrapper(env)\n",
        "  return env\n",
        "\n",
        "if 'dm_cartpole' in environment_name:\n",
        "  environment = make_environment('cartpole')\n",
        "  def render(env):\n",
        "    return env._physics.render(camera_id=0)  #pylint: disable=protected-access\n",
        "\n",
        "elif 'gym_mountaincar' in environment_name:\n",
        "  environment = gym_wrapper.GymWrapper(gym.make('MountainCarContinuous-v0'))\n",
        "  environment = wrappers.SinglePrecisionWrapper(environment)\n",
        "  def render(env):\n",
        "    return env.environment.render(mode='rgb_array')\n",
        "else:\n",
        "  raise ValueError('Unknown environment: {}.'.format(environment_name))\n",
        "\n",
        "# Show the frame.\n",
        "frame = render(environment)\n",
        "plt.imshow(frame)\n",
        "plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.5, 599.5, 399.5, -0.5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foAjaH-uypmR",
        "outputId": "72d3d536-c267-4241-fb3b-69f04d8dad0d"
      },
      "source": [
        "environment_spec = specs.make_environment_spec(environment)\n",
        "\n",
        "print('actions:\\n', environment_spec.actions, '\\n')\n",
        "print('observations:\\n', environment_spec.observations, '\\n')\n",
        "print('rewards:\\n', environment_spec.rewards, '\\n')\n",
        "print('discounts:\\n', environment_spec.discounts, '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "actions:\n",
            " BoundedArray(shape=(1,), dtype=dtype('float32'), name='action', minimum=[-1.], maximum=[1.]) \n",
            "\n",
            "observations:\n",
            " BoundedArray(shape=(2,), dtype=dtype('float32'), name='observation', minimum=[-1.2  -0.07], maximum=[0.6  0.07]) \n",
            "\n",
            "rewards:\n",
            " Array(shape=(), dtype=dtype('float32'), name='reward') \n",
            "\n",
            "discounts:\n",
            " BoundedArray(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0) \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUCiES75zY2p"
      },
      "source": [
        "#@title Definir display.\n",
        "def display_video(frames, filename='temp.mp4'):\n",
        "  \"\"\"Save and display video.\"\"\"\n",
        "  # Write video\n",
        "  with imageio.get_writer(filename, fps=60) as video:\n",
        "    for frame in frames:\n",
        "      video.append_data(frame)\n",
        "  # Read video and display the video\n",
        "  video = open(filename, 'rb').read()\n",
        "  b64_video = base64.b64encode(video)\n",
        "  video_tag = ('<video  width=\"320\" height=\"240\" controls alt=\"test\" '\n",
        "               'src=\"data:video/mp4;base64,{0}\">').format(b64_video.decode())\n",
        "  return IPython.display.HTML(video_tag)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1qerf74zcVR"
      },
      "source": [
        "#@title Run loop  { form-width: \"30%\" }\n",
        "\n",
        "\n",
        "def run_loop(environment,\n",
        "             agent,\n",
        "             num_episodes=None,\n",
        "             num_steps=None,\n",
        "             logger_time_delta=1.,\n",
        "             label='training_loop',\n",
        "             log_loss=False,\n",
        "             ):\n",
        "  \"\"\"Perform the run loop.\n",
        "\n",
        "  We are following the Acme run loop.\n",
        "\n",
        "  Run the environment loop for `num_episodes` episodes. Each episode is itself\n",
        "  a loop which interacts first with the environment to get an observation and\n",
        "  then give that observation to the agent in order to retrieve an action. Upon\n",
        "  termination of an episode a new episode will be started. If the number of\n",
        "  episodes is not given then this will interact with the environment\n",
        "  infinitely.\n",
        "\n",
        "  Args:\n",
        "    environment: dm_env used to generate trajectories.\n",
        "    agent: acme.Actor for selecting actions in the run loop.\n",
        "    num_steps: number of episodes to run the loop for. If `None` (default), runs\n",
        "      without limit.\n",
        "    num_episodes: number of episodes to run the loop for. If `None` (default),\n",
        "      runs without limit.\n",
        "    logger_time_delta: time interval (in seconds) between consecutive logging\n",
        "      steps.\n",
        "    label: optional label used at logging steps.\n",
        "  \"\"\"\n",
        "  logger = loggers.TerminalLogger(label=label, time_delta=logger_time_delta)\n",
        "  iterator = range(num_episodes) if num_episodes else itertools.count()\n",
        "  all_returns = []\n",
        "  \n",
        "  num_total_steps = 0\n",
        "  for episode in iterator:\n",
        "    # Reset any counts and start the environment.\n",
        "    start_time = time.time()\n",
        "    episode_steps = 0\n",
        "    episode_return = 0\n",
        "    episode_loss = 0\n",
        "\n",
        "    timestep = environment.reset()\n",
        "    \n",
        "    # Make the first observation.\n",
        "    agent.observe_first(timestep)\n",
        "\n",
        "    # Run an episode.\n",
        "    while not timestep.last():\n",
        "      # Generate an action from the agent's policy and step the environment.\n",
        "      action = agent.select_action(timestep.observation)\n",
        "      timestep = environment.step(action)\n",
        "\n",
        "      # Have the agent observe the timestep and let the agent update itself.\n",
        "      agent.observe(action, next_timestep=timestep)\n",
        "      agent.update()\n",
        "\n",
        "      # Book-keeping.\n",
        "      episode_steps += 1\n",
        "      num_total_steps += 1\n",
        "      episode_return += timestep.reward\n",
        "\n",
        "      if log_loss:\n",
        "        episode_loss += agent.last_loss\n",
        "\n",
        "      if num_steps is not None and num_total_steps >= num_steps:\n",
        "        break\n",
        "\n",
        "    # Collect the results and combine with counts.\n",
        "    steps_per_second = episode_steps / (time.time() - start_time)\n",
        "    result = {\n",
        "        'episode': episode,\n",
        "        'episode_length': episode_steps,\n",
        "        'episode_return': episode_return,\n",
        "    }\n",
        "    if log_loss:\n",
        "      result['loss_avg'] = episode_loss/episode_steps\n",
        "\n",
        "    all_returns.append(episode_return)\n",
        "\n",
        "    # Log the given results.\n",
        "    logger.write(result)\n",
        "    \n",
        "    if num_steps is not None and num_total_steps >= num_steps:\n",
        "      break\n",
        "  return all_returns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ38rMFA2mvp"
      },
      "source": [
        "#@title Evaluation loop { form-width: \"30%\" }\n",
        "\n",
        "def evaluate(environment, agent, evaluation_episodes):\n",
        "  frames = []\n",
        "\n",
        "  for episode in range(evaluation_episodes):\n",
        "    timestep = environment.reset()\n",
        "    episode_return = 0\n",
        "    steps = 0\n",
        "    while not timestep.last():\n",
        "      frames.append(environment.plot_state(return_rgb=True))\n",
        "\n",
        "      action = agent.select_action(timestep.observation)\n",
        "      timestep = environment.step(action)\n",
        "      steps += 1\n",
        "      episode_return += timestep.reward\n",
        "    print(\n",
        "        f'Episode {episode} ended with reward {episode_return} in {steps} steps'\n",
        "    )\n",
        "  return frames\n",
        "\n",
        "def display_video(frames, filename='temp.mp4', frame_repeat=1):\n",
        "  \"\"\"Save and display video.\"\"\"\n",
        "  # Write video\n",
        "  with imageio.get_writer(filename, fps=60) as video:\n",
        "    for frame in frames:\n",
        "      for _ in range(frame_repeat):\n",
        "        video.append_data(frame)\n",
        "  # Read video and display the video\n",
        "  video = open(filename, 'rb').read()\n",
        "  b64_video = base64.b64encode(video)\n",
        "  video_tag = ('<video  width=\"320\" height=\"240\" controls alt=\"test\" '\n",
        "               'src=\"data:video/mp4;base64,{0}\">').format(b64_video.decode())\n",
        "  return IPython.display.HTML(video_tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Jf-l5r52161"
      },
      "source": [
        "# @title Epilson-greedy policy { form-width: \"30%\" }\n",
        "def epsilon_greedy(q_values, epsilon=0.1):\n",
        "  if epsilon < np.random.random():\n",
        "    return np.argmax(q_values)\n",
        "  else:\n",
        "    return np.random.randint(np.array(q_values).shape[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bPdEe2_28bW"
      },
      "source": [
        "#@title **[Solution]** SARSA Agent { form-width: \"30%\" }\n",
        "class SarsaAgent(acme.Actor):\n",
        "\n",
        "  def __init__(\n",
        "      self, number_of_states, number_of_actions, epsilon, step_size=0.1): \n",
        "    self._q = np.zeros((number_of_states, number_of_actions))\n",
        "    self._number_of_states = number_of_states\n",
        "    self._number_of_actions = number_of_actions\n",
        "    self._step_size = step_size\n",
        "    self._epsilon = epsilon\n",
        "    self._state = None\n",
        "    self._action = None\n",
        "    self._next_state = None\n",
        "    \n",
        "  @property\n",
        "  def q_values(self):\n",
        "    return self._q\n",
        "\n",
        "  def select_action(self, observation):\n",
        "    return epsilon_greedy(self._q[observation], self._epsilon)\n",
        "    \n",
        "  def observe_first(self, timestep):\n",
        "    self._state = timestep.observation\n",
        "\n",
        "  def observe(self, action, next_timestep):\n",
        "    s = self._state\n",
        "    a = action\n",
        "    r = next_timestep.reward\n",
        "    g = next_timestep.discount\n",
        "    next_s = next_timestep.observation\n",
        "    next_a = epsilon_greedy(self._q[next_s], self._epsilon)\n",
        "    \n",
        "    # Online Q-value update\n",
        "    self._action = a\n",
        "    self._next_state = next_s\n",
        "    self._td_error = r + g * self._q[next_s, next_a] - self._q[s, a]\n",
        "\n",
        "  def update(self):\n",
        "    s = self._state\n",
        "    a = self._action\n",
        "    self._q[s, a] += self._step_size * self._td_error\n",
        "    self._state = self._next_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "5zEy-_GK4T0C",
        "outputId": "800b2078-3452-4ec7-851f-e6791a896eec"
      },
      "source": [
        "num_steps = 1e5 #@param {type:\"number\"}\n",
        "\n",
        "\n",
        "# agent \n",
        "agent = SarsaAgent(\n",
        "    number_of_states=environment_spec.observations.num_values, \n",
        "    number_of_actions=environment_spec.actions.num_values, \n",
        "    epsilon=0.1,\n",
        "    step_size=0.1)\n",
        "\n",
        "# run experiment and get the value functions from agent\n",
        "returns = run_loop(environment=environment, agent=agent, num_steps=int(num_steps))\n",
        "\n",
        "# get the q-values\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "\n",
        "# visualize value functions\n",
        "print('AFTER {} STEPS ...'.format(num_steps))\n",
        "plot_action_values(q, epsilon=1.)\n",
        "\n",
        "# visualise the greedy policy\n",
        "grid.plot_greedy_policy(q)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-f6e5a2a08892>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m agent = SarsaAgent(\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mnumber_of_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menvironment_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mnumber_of_actions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menvironment_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'BoundedArray' object has no attribute 'num_values'"
          ]
        }
      ]
    }
  ]
}